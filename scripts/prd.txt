# Markdown Legal Document Processing Pipeline

## 1. Introduction
This document outlines the requirements for a new pipeline to process legal documents provided in Markdown format. The goal is to extract structured information from these documents, specifically by processing each section (identified by headings) independently, and then integrating this information into an Elasticsearch-compatible format.

## 2. Goals
- Automate the parsing of Markdown legal documents.
- Enable section-by-section processing for improved accuracy and context.
- Generate structured JSON output suitable for Elasticsearch indexing.
- Utilize Bun runtime for performance.

## 3. Current State
- `ingestion-pipeline/parse.ts`: Existing script for parsing a full document into a single JSON object.
- `ingestion-pipeline/markdown-parser.ts`: Existing script to parse Markdown into a hierarchical JSON structure.

## 4. Proposed Solution

### 4.1. Input
The pipeline will accept a Markdown file (e.g., `output.md`) as input.

### 4.2. Step 1: Markdown to Hierarchy JSON
- The `ingestion-pipeline/markdown-parser.ts` script will be used to convert the input Markdown file into a hierarchical JSON structure.
- The output JSON (e.g., `hierarchy.json`) should be saved in a *unique folder* for each processing run. This folder should be named based on a timestamp or a unique identifier to prevent overwriting previous runs.

### 4.3. Step 2: Section-wise Parsing to Elasticsearch Format
- An adaptation of `ingestion-pipeline/parse.ts` will be created. Let's call it `ingestion-pipeline/parse-sections.ts`.
- Instead of calling `generateObject` for the entire `fileContent`, `parse-sections.ts` will iterate through each heading object in the `hierarchy.json` generated in Step 1.
- For each heading object (representing a section or article), `generateObject` will be called to produce an `ElasticsearchArticleSchema` compliant JSON object.
- The `systemPrompt` and `userPrompt` within `parse-sections.ts` will need to be adjusted to reflect that it's processing a *section* of a document, not the entire document. The `userPrompt` should pass the content of the specific section.
- The `LawParseOutputSchema` will need to be adapted or a new schema created to handle the fact that we are processing individual sections. The `law_unique_id`, `law_full_title`, etc., will need to be passed or inferred for each section's processing, or handled as part of a larger aggregation step. For now, assume these global fields will be part of the final aggregation. The immediate output of `parse-sections.ts` for each section should be an `ElasticsearchArticleSchema` object.
- The output of this step will be a collection of `ElasticsearchArticleSchema` objects, one for each processed section. These should also be saved in the unique folder created in Step 1, perhaps as an array in a single JSON file (e.g., `parsed_sections.json`).

## 5. Technical Considerations
- **Runtime:** Bun will be used for all scripts.
- **Dependencies:** Ensure all necessary `bun install` commands are documented or automated.
- **Error Handling:** Robust error handling should be implemented for file operations and LLM calls.
- **Output Management:** The unique folder creation and file saving should be robust.

## 6. Future Enhancements (Out of Scope for initial PRD)
- Aggregation of section-wise outputs into a single `LawParseOutputSchema` compliant document.
- Integration with a queueing system for batch processing.
- UI for input and output management.
